# HashTable
> Our goal is to get the best result with the minimum of effort.

### Hardcoding and assembling
By analizing the output of perf, I realized the the significant amount of cpu time is spent on hash function. So, I decided to take it out of template parameter and hardcode in table with inline assembly. To make it work even better, I used the version generated by g++ with -O3 flag.
```C++
unsigned long hash(const char* key){
        unsigned long d = 0;

        asm volatile(R"(
            .intel_syntax noprefix
    
            movsx   rax, BYTE PTR [rdi]
            test    al, al
            je      .L4
            xor     edx, edx
    .L3:
            add     rax, rdx
            add     rdi, 1
            mov     rdx, rax
            sal     rdx, 10
            add     rax, rdx
            mov     rdx, rax
            shr     rdx, 6
            xor     rdx, rax
            movsx   rax, BYTE PTR [rdi]
            test    al, al
            jne     .L3
            lea     rdx, [rdx+rdx*8]
            mov     rax, rdx
            shr     rax, 11
            xor     rax, rdx
            mov     rdx, rax
            sal     rdx, 15
            add     rax, rdx
            jmp .Exit
    
    .L4:
            xor     eax, eax
    .Exit:
            .att_syntax
    )"
        :"=r"(h)
        :"D"(key)
    );

        return h;

}
```
Overall performance improved by 5%

### Selecting Hash Function
Jenkins hash is quite good, as you can see in the diagram above, hovewer it is not the fastest one. So I decided to use a crc32 hash, because of it's great distribution characteristic and hardware support.
```C++
inline unsigned long hash(const char* data){

    unsigned long h = 0;
    asm(R"(
        .intel_syntax noprefix
        lea rax, [%1]
        xor %0, %0
    hashing:
        crc32 %0, byte ptr [rax]
        inc rax
        cmp byte ptr [rax], 0
        jne hashing
        .att_syntax prefix
    )"
        : "=r"(h)
        : "r"(data)
        : "rax", "rcx"
	);    

    return h;
}

```
Now overall performance improved by 10%

### Improving strcmp
```C++
int ull_cmp(const char* a, const char* b){

	unsigned long long a_chunk[4] = {0};
	unsigned long long b_chunk[4] = {0};

	for (int i = 0; i < 4; ++i){
		a_chunk[i] = *reinterpret_cast<const unsigned long long*>(a);
		b_chunk[i] = *reinterpret_cast<const unsigned long long*>(b);

		a += 8;
		b += 8;

		if ((a_chunk[i] ^ b_chunk[i]) != 0) return -1;
	}

	return 0;
}

```C++
int ull_cmp(const char* a, const char* b){
    int d = -1;

    asm (R"(

        .intel_syntax noprefix
        mov     rax, QWORD PTR [rdi]
        cmp     QWORD PTR [rsi], rax
        jne     different 
        mov     rax, QWORD PTR [rsi+8]
        cmp     QWORD PTR [rdi+8], rax
        jne     different 
        mov     rax, QWORD PTR [rsi+16]
        cmp     QWORD PTR [rdi+16], rax
        jne     different 
        mov     rax, QWORD PTR [rsi+24]
        cmp     QWORD PTR [rdi+24], rax
        jne     different 

        mov %0, 0
        jmp exit

    different:
            mov %0, -1;

    exit:
        .att_syntax
        )"
        :"=r"(d)
        :"D"(a), "S"(b)
        );

    return d;
}
```
---
```C+++
int avx_cmp(const char *a, const char *b) {

    __m128i xmm0, xmm1;
    unsigned int eax;

    xmm0 = _mm_loadu_si128((__m128i*)(a));
    xmm1 = _mm_loadu_si128((__m128i*)(b));
    xmm0 = _mm_cmpeq_epi8(xmm0, xmm1);
    eax = _mm_movemask_epi8(xmm0);

    if (eax == 0xffff) return 0;
        return -1; 
}
```

```C++
inline int avx_cmp(const char *a, const char *b) {

    int d = -1;

    asm (R"(
        .intel_syntax noprefix
        movdqu xmm0, XMMWORD PTR [%1]
        movdqu xmm1, XMMWORD PTR [%2]
        pcmpeqb xmm0, xmm1
        pmovmskb eax, xmm0
        cmp eax, 0xffff
        setne al
        movzx eax, al
        neg eax
        mov %0, eax
        .att_syntax
    )"
        :"=r"(d)
        :"r"(a), "r"(b)
        :"eax", "xmm0", "xmm1"
        );

    return d;
}
```
